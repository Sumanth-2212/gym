"""
Comprehensive Test Runner for All AI Models
Executes all test suites and generates detailed reports
"""

import os
import sys
import json
import pytest
from datetime import datetime
from pathlib import Path
import subprocess
from typing import Dict, List
import pandas as pd

class AIModelTestRunner:
    """Orchestrates testing for all AI models"""
    
    def __init__(self):
        self.test_results = {}
        self.report_dir = Path("tests/reports")
        self.report_dir.mkdir(exist_ok=True)
        
        self.models = [
            {
                "name": "Pose Checker",
                "test_module": "tests.pose_checker.test_pose_model",
                "min_pass_rate": 0.90
            },
            {
                "name": "Workout Recommender",
                "test_module": "tests.workout_recommender.test_workout_model",
                "min_pass_rate": 0.95
            },
            {
                "name": "Nutrition Planner",
                "test_module": "tests.nutrition_planner.test_nutrition_model",
                "min_pass_rate": 0.95
            },
            {
                "name": "Fitness Chatbot",
                "test_module": "tests.chatbot.test_chatbot_model",
                "min_pass_rate": 0.90
            }
        ]
    
    def run_all_tests(self):
        """Execute all test suites"""
        print("="*80)
        print("üöÄ AI MODEL QUALITY ASSURANCE TEST SUITE")
        print("="*80)
        print(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Environment: {sys.platform}")
        print("="*80)
        
        overall_results = {
            "test_date": datetime.now().isoformat(),
            "models_tested": [],
            "total_tests": 0,
            "total_passed": 0,
            "total_failed": 0,
            "overall_pass_rate": 0.0
        }
        
        for model in self.models:
            print(f"\nüìä Testing {model['name']}...")
            print("-"*60)
            
            result = self.run_model_tests(model)
            overall_results["models_tested"].append(result)
            overall_results["total_tests"] += result["total_tests"]
            overall_results["total_passed"] += result["passed"]
            overall_results["total_failed"] += result["failed"]
            
            self.print_model_results(model['name'], result)
        
        # Calculate overall pass rate
        if overall_results["total_tests"] > 0:
            overall_results["overall_pass_rate"] = (
                overall_results["total_passed"] / overall_results["total_tests"]
            )
        
        # Generate comprehensive report
        self.generate_comprehensive_report(overall_results)
        
        # Print summary
        self.print_summary(overall_results)
        
        return overall_results
    
    def run_model_tests(self, model: Dict) -> Dict:
        """Run tests for a specific model"""
        
        # Create test report file
        report_file = self.report_dir / f"{model['name'].lower().replace(' ', '_')}_report.json"
        
        # Run pytest with JSON report
        pytest_args = [
            "-v",
            "-q",
            "--tb=short",
            f"--json-report",
            f"--json-report-file={report_file}",
            model["test_module"]
        ]
        
        # Execute tests
        try:
            # For this mock implementation, we'll simulate test results
            result = self.simulate_test_execution(model)
        except Exception as e:
            print(f"Error running tests: {e}")
            result = {
                "model": model["name"],
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0,
                "pass_rate": 0.0,
                "error": str(e)
            }
        
        return result
    
    def simulate_test_execution(self, model: Dict) -> Dict:
        """Simulate test execution (for demonstration)"""
        import random
        
        # Simulate test results
        total_tests = 50
        
        # Generate realistic pass rates based on model
        if model["name"] == "Pose Checker":
            passed = random.randint(43, 48)
        elif model["name"] == "Workout Recommender":
            passed = random.randint(45, 49)
        elif model["name"] == "Nutrition Planner":
            passed = random.randint(46, 50)
        else:  # Chatbot
            passed = random.randint(44, 48)
        
        failed = total_tests - passed
        
        # Generate detailed test results
        test_details = []
        for i in range(total_tests):
            test_details.append({
                "test_id": f"{model['name'][:4].upper()}_{i+1:03d}",
                "test_name": f"test_case_{i+1}",
                "status": "PASS" if i < passed else "FAIL",
                "duration": random.uniform(0.1, 2.0),
                "message": "" if i < passed else f"Assertion failed at step {random.randint(1, 5)}"
            })
        
        return {
            "model": model["name"],
            "total_tests": total_tests,
            "passed": passed,
            "failed": failed,
            "skipped": 0,
            "pass_rate": passed / total_tests,
            "min_required_rate": model["min_pass_rate"],
            "meets_threshold": (passed / total_tests) >= model["min_pass_rate"],
            "test_details": test_details,
            "execution_time": random.uniform(10, 30)
        }
    
    def print_model_results(self, model_name: str, result: Dict):
        """Print results for a specific model"""
        
        print(f"\n‚úÖ Test Results for {model_name}:")
        print(f"   Total Tests: {result['total_tests']}")
        print(f"   Passed: {result['passed']} ‚úÖ")
        print(f"   Failed: {result['failed']} ‚ùå")
        print(f"   Pass Rate: {result['pass_rate']*100:.2f}%")
        print(f"   Required: {result['min_required_rate']*100:.2f}%")
        
        if result['meets_threshold']:
            print(f"   Status: MEETS REQUIREMENTS ‚úÖ")
        else:
            print(f"   Status: BELOW THRESHOLD ‚ö†Ô∏è")
        
        print(f"   Execution Time: {result['execution_time']:.2f}s")
    
    def generate_comprehensive_report(self, results: Dict):
        """Generate comprehensive HTML and JSON reports"""
        
        # Save JSON report
        json_report_path = self.report_dir / "comprehensive_test_report.json"
        with open(json_report_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # Generate HTML report
        html_report = self.generate_html_report(results)
        html_report_path = self.report_dir / "test_report.html"
        with open(html_report_path, 'w') as f:
            f.write(html_report)
        
        # Generate CSV summary
        self.generate_csv_summary(results)
        
        print(f"\nüìÅ Reports saved to: {self.report_dir}")
    
    def generate_html_report(self, results: Dict) -> str:
        """Generate HTML test report"""
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AI Model Test Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1 {{ color: #333; }}
                .summary {{ background: #f0f0f0; padding: 15px; border-radius: 5px; }}
                .pass {{ color: green; font-weight: bold; }}
                .fail {{ color: red; font-weight: bold; }}
                .warning {{ color: orange; font-weight: bold; }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background: #4CAF50; color: white; }}
                tr:nth-child(even) {{ background: #f2f2f2; }}
                .chart {{ margin: 20px 0; }}
            </style>
        </head>
        <body>
            <h1>ü§ñ AI Model Quality Assurance Report</h1>
            <div class="summary">
                <h2>Executive Summary</h2>
                <p><strong>Test Date:</strong> {results['test_date']}</p>
                <p><strong>Total Models Tested:</strong> {len(results['models_tested'])}</p>
                <p><strong>Total Test Cases:</strong> {results['total_tests']}</p>
                <p><strong>Overall Pass Rate:</strong> 
                    <span class="{'pass' if results['overall_pass_rate'] >= 0.9 else 'fail'}">
                        {results['overall_pass_rate']*100:.2f}%
                    </span>
                </p>
            </div>
            
            <h2>üìä Model-wise Results</h2>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Total Tests</th>
                    <th>Passed</th>
                    <th>Failed</th>
                    <th>Pass Rate</th>
                    <th>Required</th>
                    <th>Status</th>
                </tr>
        """
        
        for model_result in results['models_tested']:
            status_class = 'pass' if model_result['meets_threshold'] else 'fail'
            status_text = '‚úÖ PASS' if model_result['meets_threshold'] else '‚ùå FAIL'
            
            html += f"""
                <tr>
                    <td>{model_result['model']}</td>
                    <td>{model_result['total_tests']}</td>
                    <td>{model_result['passed']}</td>
                    <td>{model_result['failed']}</td>
                    <td>{model_result['pass_rate']*100:.2f}%</td>
                    <td>{model_result['min_required_rate']*100:.2f}%</td>
                    <td class="{status_class}">{status_text}</td>
                </tr>
            """
        
        html += """
            </table>
            
            <h2>üìà Detailed Test Metrics</h2>
            <div class="chart">
                <h3>Pass/Fail Distribution</h3>
                <canvas id="passFailChart"></canvas>
            </div>
            
            <h2>üîç Key Findings</h2>
            <ul>
        """
        
        # Add key findings
        for model_result in results['models_tested']:
            if not model_result['meets_threshold']:
                html += f"""
                    <li class="warning">
                        {model_result['model']} is below required threshold 
                        ({model_result['pass_rate']*100:.1f}% vs {model_result['min_required_rate']*100:.1f}% required)
                    </li>
                """
        
        html += """
            </ul>
            
            <h2>üí° Recommendations</h2>
            <ul>
                <li>Review failed test cases and identify patterns</li>
                <li>Enhance model training for edge cases</li>
                <li>Implement additional validation layers</li>
                <li>Consider A/B testing for model improvements</li>
            </ul>
            
            <script>
                // Add chart visualization here if needed
            </script>
        </body>
        </html>
        """
        
        return html
    
    def generate_csv_summary(self, results: Dict):
        """Generate CSV summary of test results"""
        
        data = []
        for model_result in results['models_tested']:
            data.append({
                'Model': model_result['model'],
                'Total Tests': model_result['total_tests'],
                'Passed': model_result['passed'],
                'Failed': model_result['failed'],
                'Pass Rate': f"{model_result['pass_rate']*100:.2f}%",
                'Required Rate': f"{model_result['min_required_rate']*100:.2f}%",
                'Status': 'PASS' if model_result['meets_threshold'] else 'FAIL',
                'Execution Time (s)': f"{model_result['execution_time']:.2f}"
            })
        
        df = pd.DataFrame(data)
        csv_path = self.report_dir / "test_summary.csv"
        df.to_csv(csv_path, index=False)
        
        return csv_path
    
    def print_summary(self, results: Dict):
        """Print final test summary"""
        
        print("\n" + "="*80)
        print("üìä FINAL TEST SUMMARY")
        print("="*80)
        
        print(f"\nüéØ Overall Statistics:")
        print(f"   ‚Ä¢ Models Tested: {len(results['models_tested'])}")
        print(f"   ‚Ä¢ Total Test Cases: {results['total_tests']}")
        print(f"   ‚Ä¢ Tests Passed: {results['total_passed']}")
        print(f"   ‚Ä¢ Tests Failed: {results['total_failed']}")
        print(f"   ‚Ä¢ Overall Pass Rate: {results['overall_pass_rate']*100:.2f}%")
        
        print(f"\nüìà Model Performance:")
        for model_result in results['models_tested']:
            status = "‚úÖ" if model_result['meets_threshold'] else "‚ö†Ô∏è"
            print(f"   {status} {model_result['model']}: {model_result['pass_rate']*100:.2f}%")
        
        # Determine overall status
        all_passed = all(m['meets_threshold'] for m in results['models_tested'])
        
        print("\n" + "="*80)
        if all_passed:
            print("‚úÖ ALL MODELS MEET QUALITY THRESHOLDS")
            print("üéâ READY FOR PRODUCTION DEPLOYMENT")
        else:
            print("‚ö†Ô∏è SOME MODELS BELOW THRESHOLD")
            print("üîß ADDITIONAL OPTIMIZATION REQUIRED")
        print("="*80)
        
        print(f"\nüìÅ Detailed reports available in: {self.report_dir}")
        print("   ‚Ä¢ comprehensive_test_report.json")
        print("   ‚Ä¢ test_report.html")
        print("   ‚Ä¢ test_summary.csv")

def main():
    """Main test execution"""
    runner = AIModelTestRunner()
    results = runner.run_all_tests()
    
    # Return exit code based on results
    all_passed = all(m['meets_threshold'] for m in results['models_tested'])
    sys.exit(0 if all_passed else 1)

if __name__ == "__main__":
    main()